{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Génération de légendes d'images automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etudiant : CHIBAN Amine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cours : PGM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectif :  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un modèle capable de générer une légende d'une image quelconque "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vu d'ensemble :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utiliser un réseaux de neurones à convolution (CNN) pour extraire les features des images\n",
    "- Utiliser un réseaux de neurones récurents (RNN) pour générer les mots de la description de manière séquentielle\n",
    "- Ajouter un mécanisme d'attention au réseau de neurones récurents pour améliorer les prédictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](notebook-img/over.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix du CNN que nous allons utiliser : VGG16, 16 représente le nombre de couche de notre réseau de neurones à convolution. Une série de 5 bloc (couches de convolution et max pooling) pour l'extraction de features puis 3 couches pour la classification. Comme le montre l'image suivante :\n",
    "![title](notebook-img/picture.png)\n",
    "- 3x3 Conv 64 : convolution par 64 filtres de taille 3x3.\n",
    "- pool/2 : fonction max pooling, pour une fenetre de 2x2. Remplacer une fenetre 2x2 par la valeur du max et donc devient 1x1.\n",
    "- FC 4096 : Couche de réseau de neurones simple avec 4096 noeuds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le temps d'apprentissage du VGG étant très grands pour une base de données suffisante au bon apprentissage des features (plus de 2 jours avec des performances élevées), nous allons utilisée un modéle pré entrainé du VGG disponible sur pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu que nous n'avons pas besoin de la partie classification, nous allons devoir enveler la dernière brique contenant le réseau de neurones simple (la brique contenant les FC 4096). Nous aurons donc un éxtracteur de features que nous pourons utiliser comme entrées dans la deuxième partie, qui est dédiée à la génération de descriptions à partir des features de l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.transform import resize\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder class to transform images into features. We're using the features from the last conv layer.\n",
    "    So the output will be 14x14x512\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Imgs must be normalised before being fed to the CNN as per de docs\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.transform=transforms.Compose([normalize])\n",
    "        \n",
    "        # Using the official VGG16 model pretrained\n",
    "        vgg = torchvision.models.vgg16(pretrained=True)  \n",
    "        \n",
    "        # Remove the classification bloc\n",
    "        modules = list(vgg.children())[:-1]\n",
    "\n",
    "        # Remove the last maxpool layer\n",
    "        modules[0]=nn.Sequential(*list(modules[0].children())[:-1])\n",
    "\n",
    "        # Save the new model \n",
    "        self.vgg = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Params:\n",
    "            - images : tensor of input images (batch_size, 3, 224, 224)\n",
    "        Return:\n",
    "            - output : Tensor of features (14,14,512)\n",
    "        \"\"\"\n",
    "        output = self.transform(images.squeeze())\n",
    "        output = self.vgg(images)  \n",
    "        output = output.permute(0, 2, 3, 1)  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "from skimage.transform import resize\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_flickr_data():\n",
    "    \"\"\"\n",
    "    This function is used to generate input files from raw data files. It generates 3 types of files:\n",
    "\n",
    "    Images : we can either load them raw, vector h5py or generate the features using the encoder\n",
    "    Captions : We sample a number of captions per image then store them as Json\n",
    "    Caption lenghts : the lenghts of each captions, usefull to know when to stop the process ?!\n",
    "\n",
    "    The data is also split into train evaluate and test\n",
    "    \"\"\"\n",
    "\n",
    "    min_frequency = 2\n",
    "    max_cap_len=20\n",
    "    output_folder='./processed_data'\n",
    "    caps_per_img=2\n",
    "\n",
    "    # Loading split IDs\n",
    "    train_ids=load_doc('./raw_data/Flickr8k_text/Flickr8k.trainImages.txt')\n",
    "    train_ids=[x.split('.')[0] for x in train_ids.split('\\n')]\n",
    "    eval_ids=load_doc('./raw_data/Flickr8k_text/Flickr8k.devImages.txt')\n",
    "    eval_ids=[x.split('.')[0] for x in eval_ids.split('\\n')]\n",
    "    test_ids=load_doc('./raw_data/Flickr8k_text/Flickr8k.testImages.txt')\n",
    "    test_ids=[x.split('.')[0] for x in test_ids.split('\\n')]\n",
    "\n",
    "    # Generating proccessed images then storing them\n",
    "    for ID in train_ids[:20]:\n",
    "        image=proccess_image('./raw_data/Flickr8k_data/'+ID+'.jpg')\n",
    "        torch.save(image, './processed_data/train_images/'+ID+'.pt')\n",
    "        print('Train image',ID,'Generated')\n",
    "    for ID in eval_ids[:20]:\n",
    "        image=proccess_image('./raw_data/Flickr8k_data/'+ID+'.jpg')\n",
    "        torch.save(image, './processed_data/eval_images/'+ID+'.pt')\n",
    "        print('Validation Image',ID,'Generated')\n",
    "    for ID in test_ids[:20]:\n",
    "        image=proccess_image('./raw_data/Flickr8k_data/'+ID+'.jpg')\n",
    "        torch.save(image, './processed_data/test_images/'+ID+'.pt')\n",
    "        print('Test Image',ID,'Generated')\n",
    "\n",
    "    # Loading captions\n",
    "    data=load_doc('./raw_data/Flickr8k_text/Flickr8k.token.txt')\n",
    "    train_captions,eval_captions,test_captions=load_captions(data,train_ids,eval_ids,test_ids,caps_per_img)\n",
    "    \n",
    "    # Generating the wordmap then saving it to file\n",
    "    wordmap=generate_wordmap([train_captions,eval_captions,test_captions],min_frequency)\n",
    "    with open(os.path.join(output_folder, 'WORDMAP.json'), 'w') as j:\n",
    "        json.dump(wordmap, j)\n",
    "\n",
    "    # Process captions then store in file\n",
    "    train_captions,eval_captions,test_captions=process_captions([train_captions,eval_captions,test_captions],wordmap,max_cap_len)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_CAPTIONS.json'), 'w') as j:\n",
    "        json.dump(train_captions, j)\n",
    "    with open(os.path.join(output_folder, 'EVAL_CAPTIONS.json'), 'w') as j:\n",
    "        json.dump(eval_captions, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_CAPTIONS.json'), 'w') as j:\n",
    "        json.dump(train_captions, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_wordmap(splits,min_frequency):\n",
    "    word_counter=Counter()\n",
    "    for split in splits:\n",
    "        for line in split.values():\n",
    "            for cap in line:\n",
    "                word_counter.update(cap.split(' '))\n",
    "    words= [ x for x  in word_counter.keys() if word_counter[x]>min_frequency]\n",
    "    wordmap = {k: v + 1 for v, k in enumerate(words)}\n",
    "    wordmap['<unk>'] = len(wordmap) + 1\n",
    "    wordmap['<start>'] = len(wordmap) + 1\n",
    "    wordmap['<end>'] = len(wordmap) + 1\n",
    "    wordmap['<pad>'] = 0    \n",
    "    return wordmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_captions(data,train_ids,eval_ids,test_ids,caps_per_img):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    train_captions = {}\n",
    "    eval_captions = {}\n",
    "    test_captions = {}\n",
    "    for line in data.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_cap = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_cap=[w.translate(table) for w in image_cap]\n",
    "        #image_cap=[w for w in image_cap if len(w)>1]\n",
    "        image_cap = ' '.join(image_cap).lower()\n",
    "        if image_id in train_ids:\n",
    "            if image_id not in train_captions.keys():\n",
    "                train_captions[image_id] = []\n",
    "            if (len(train_captions[image_id])<=caps_per_img):\n",
    "                train_captions[image_id].append(image_cap)\n",
    "\n",
    "        if image_id in eval_ids:\n",
    "            if image_id not in eval_captions.keys():\n",
    "                eval_captions[image_id] = []\n",
    "            if (len(eval_captions[image_id])<=caps_per_img):\n",
    "                eval_captions[image_id].append(image_cap)\n",
    "\n",
    "        if image_id in test_ids:\n",
    "            if image_id not in test_captions.keys():\n",
    "                test_captions[image_id] = []\n",
    "            if (len(test_captions[image_id])<=caps_per_img):\n",
    "                test_captions[image_id].append(image_cap)\n",
    "\n",
    "    return train_captions,eval_captions,test_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t\"\"\"\n",
    "    Helper function to load a file as a string\n",
    "    \"\"\"\n",
    "\tfile = open(filename, 'r')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proccess_image(path):\n",
    "    img = io.imread(path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = resize(img, (256, 256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img=torch.Tensor(img)\n",
    "    img=img.reshape(1,3,img.shape[1],img.shape[2])\n",
    "    enc=Encoder()\n",
    "    return enc.forward(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de lancer la fonction suivante, il faut avoir le dataset flickr 8k avec les 5 Captions pour chacune des images, déposée dans un dossier row_data suivant le chemin indiqué dans la fonction preprocess_flickr_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess_flickr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vue du temps nécessaire pour traiter toutes les images, et pour l'entrainement du LSTM et le mécanisme d'attention plus loin, j'ai décidé d'utiliser 20 images seulement.\n",
    "Au niveau du code de la fonction preprocess_flickr_data(), j'ai mis : for ID in train_ids[:20]. Et donc on ne prend que 20 images du dataset d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous aurons donc comme résultats un dossier nomé processed_data contenant : \n",
    "- WORDMAP.json : le vocabulaire que notre systeme connait\n",
    "- TRAIN_CAPTIONS.json : les captions pour les images d'entrainement avec l'indice de chaque mot de le WORDMAP\n",
    "- EVAL_CAPTIONS.json : les captions pour les images d'évalutation avec l'indice de chaque mot de le WORDMAP\n",
    "- TEST_CAPTIONS.json : les captions pour les images de test avec l'indice de chaque mot de le WORDMAP\n",
    "- eval_images : images du dataset d'évaluation encodées\n",
    "- train_images : images du dataset d'entrainement encodées\n",
    "- test_images : images du dataset de test encodées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mécanisme d'attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the attention model. \n",
    "    It is based on 1 linear layer folowed by tanh then a softmax to compute the weights,\n",
    "    Then outputs the features*weights , weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lstm_len, attention_len, features_len=512):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            - features_len : size of the images feature vector we're using 512\n",
    "            - lstm_len : size of the lstm network (it's the size of the hidden and cell states also)\n",
    "            - attention_len : size of the attention network (Not sure how it affects the learning yet)\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # Simple linear layer to project the encoder features\n",
    "        self.image_layer = nn.Linear(features_len, 1) \n",
    "        # Simple linear layer to project the hidden state \n",
    "        self.hidden_layer = nn.Linear(lstm_len, 1)  \n",
    "        # A Tanh layer\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Simple linear layer to merge the two vectors\n",
    "        self.merge_layer = nn.Linear(attention_len, 1)  \n",
    "        # Softmax layer to compure weights\n",
    "        self.softmax = nn.Softmax(dim=1)  \n",
    "\n",
    "    def forward(self, image_features, hidden_state): \n",
    "        \"\"\"\n",
    "        Forward propagation. Computes the attention weights using the equation\n",
    "\n",
    "                        Si=tanh(Wc*C+Wx*Xi) , weights=softmax(si)\n",
    "\n",
    "        Params:\n",
    "            - image_features : vector of image features (batch_size,14,14,512)\n",
    "            - hidden_state : vector Ht-1 previous hidden state (batch_size,lstm_len)\n",
    "\n",
    "        Output:\n",
    "            - attention_features : weights*features (batchsize,lstm_len)\n",
    "            - attention_weights : weights computed with softmax (batchsize,nb_image_sections=14*14)\n",
    "        \"\"\"\n",
    "        features = self.image_layer(image_features)  \n",
    "        hidden = self.hidden_layer(hidden_state)  \n",
    "        merged = features + hidden.unsqueeze(1)\n",
    "        tanh = self.tanh(merged)\n",
    "        attention_weights = self.softmax(tanh).squeeze(2) \n",
    "        attention_features = (image_features * attention_weights.unsqueeze(2)).sum(dim=1) \n",
    "\n",
    "        return attention_features, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour loader les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Helper class to load up the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,data_folder,split,caps_per_img):\n",
    "\n",
    "        self.split = split\n",
    "        self.data_folder=data_folder\n",
    "        self.caps_per_img=caps_per_img\n",
    "\n",
    "        # Load captions\n",
    "        with open(os.path.join(data_folder, self.split + '_CAPTIONS.json'), 'r') as j:\n",
    "            self.captions = json.load(j)\n",
    "\n",
    "        # Length of the dataset\n",
    "        self.dataset_size = len(self.captions)*caps_per_img\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        if self.split=='DEV':\n",
    "            self.split='TRAIN'\n",
    "            \n",
    "        ID=list(self.captions.keys())[i//self.caps_per_img]\n",
    "        img_path=self.data_folder+'/'+self.split+'_images/'+ID+'.pt'\n",
    "        img=torch.load(img_path)\n",
    "        \n",
    "        caption = torch.Tensor(self.captions[ID][\"caps\"][i%2]).to(dtype=torch.int16)\n",
    "        caplen = torch.Tensor([self.captions[ID][\"caplens\"][i%2]]).to(dtype=torch.int16)\n",
    "\n",
    "        return img, caption,caplen\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the decoder model. \n",
    "    it's based on an lstm and uses attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_len, embedding_len, features_len, wordmap_len, lstm_len=512):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            - attention_len: size of attention network\n",
    "            - embedding_len: size of the embedding vector\n",
    "            - features_len: size of the images features (14,14,512)\n",
    "            - wordmap_len: number of words in the vocabulary\n",
    "            - lstm_len: size of the lstm network\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.lstm_len = lstm_len\n",
    "        self.attention_len = attention_len\n",
    "        self.embedding_len = embedding_len\n",
    "        self.features_len = features_len\n",
    "        self.wordmap_len = wordmap_len\n",
    "\n",
    "        # Our attention model\n",
    "        self.attention = Attention(lstm_len, features_len, attention_len)\n",
    "        # Embedding model, it transforms each word vector into an embedding vector\n",
    "        self.embedding = nn.Embedding(wordmap_len, embedding_len)  \n",
    "        # LSTM model. We use LSTMCell and implement the loop manualy to use attention\n",
    "        self.lstm = nn.LSTMCell(embedding_len + lstm_len, features_len, bias=True) \n",
    "        # A simple linear layer to compute the vocabulary scores from the hidden state\n",
    "        self.scoring_layer = nn.Linear(features_len, wordmap_len)\n",
    "\n",
    "    def init_hidden_state(self, image_features):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state and cell state for each sentence.\n",
    "        Using zero tensors for now, might change later.\n",
    "\n",
    "        Params:\n",
    "            - image_features : vector of image features (batch_size,14,14,512)\n",
    "\n",
    "        Output:\n",
    "            - Hidden state : vector for initial hidden state\n",
    "            - Cell state : vector for initial cell state\n",
    "        \"\"\"\n",
    "        h = torch.zeros((image_features.shape[0],image_features.shape[-1]))  # (batch_size, features_len)\n",
    "        c = torch.zeros((image_features.shape[0],image_features.shape[-1]))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, image_features, caps, caplens):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Params:\n",
    "            - image_features : vector of image features (batch_size,14,14,512)\n",
    "            - caps: image captions (batch_size, max_caption_length)\n",
    "            - caplens: image caption lengths (batch_size, 1)\n",
    "        Output:\n",
    "            - scores : scores for each word (batch_size, wordmap_len)\n",
    "            - caps_sorted : a sorted list of caps by lenghts.\n",
    "            - decode lengths : caplens - 1\n",
    "            - weights : attention weights\n",
    "            - sort indices : can be used later\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = image_features.size(0)\n",
    "        lstm_len = image_features.size(-1)\n",
    "        wordmap_len = self.wordmap_len\n",
    "\n",
    "        # Flatten image 14*14 -> 196\n",
    "        image_features = image_features.view(batch_size, -1, lstm_len)\n",
    "        num_pixels = image_features.size(1)\n",
    "\n",
    "        # Sort the sentences by decreasing lenght, so we can decode only the k first sentences that haven't\n",
    "        # reached <end> yet. We can use index to select them, but this is cleaner\n",
    "        caplens, sort_ind = caplens.squeeze(1).sort(dim=0, descending=True)\n",
    "        image_features = image_features[sort_ind]\n",
    "        caps = caps[sort_ind]\n",
    "        \n",
    "        # Transforms the captions into embedding vectors\n",
    "        embeddings = self.embedding(caps)  \n",
    "        # Initialize LSTM  hidden and cell state\n",
    "        h, c = self.init_hidden_state(image_features)  \n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caplens - 1).tolist()\n",
    "\n",
    "        # Result tensors\n",
    "        scores = torch.zeros(batch_size, max(decode_lengths), self.wordmap_len)\n",
    "        weights = torch.zeros(batch_size, max(decode_lengths), num_pixels)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lengths)):\n",
    "\n",
    "            # At the moment t we only decode the sentences that haven't reached <end>, so the K first sentences\n",
    "            # Since they are sorted by lenght we can just use [:K]\n",
    "            k = sum([l > t for l in decode_lengths])\n",
    "\n",
    "            # We first generate the attention weighted images. Alpha is the weights of the attention model.\n",
    "            attention_encoding, alpha = self.attention(image_features[:k],h[:k])\n",
    "\n",
    "            # Concatenate Previous word + features\n",
    "            decode_input=torch.cat([embeddings[:k, t, :],attention_encoding], dim=1)\n",
    "            \n",
    "            # We run the LSTM cell using the decode imput and (hidden,cell) states\n",
    "            h, c = self.lstm(decode_input, (h[:k],c[:k]) ) \n",
    "\n",
    "            # The hidden state is transformed into vocabulary scores by a simple linear layer\n",
    "            score = self.scoring_layer(h)  # (k, wordmap_len)\n",
    "\n",
    "            # Finaly we store the scores and weights\n",
    "            scores[:k, t, :] = score\n",
    "            weights[:k, t, :] = alpha\n",
    "\n",
    "        return scores, caps, decode_lengths, weights, sort_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- LEARNING STARTED -------------------------------------------\n",
      "----------------------------------- Epoch 1 ----------------------------------\n",
      "Prediction :   a a dog a a a a a a a\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 2 ----------------------------------\n",
      "Prediction :   a a a a a a a a a a\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 3 ----------------------------------\n",
      "Prediction :   a a dog a a a a a a a\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 4 ----------------------------------\n",
      "Prediction :   a dog dog dog a dog dog dog a the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 5 ----------------------------------\n",
      "Prediction :   a dog dog is a dog dog a a the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 6 ----------------------------------\n",
      "Prediction :   a dog dog is a dog dog running a the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 7 ----------------------------------\n",
      "Prediction :   a brown dog is a brown dog running a the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 8 ----------------------------------\n",
      "Prediction :   a brown dog is a black dog running the the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 9 ----------------------------------\n",
      "Prediction :   a brown dog is a black dog running across the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 10 ----------------------------------\n",
      "Prediction :   a brown dog is a black collar running across the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 11 ----------------------------------\n",
      "Prediction :   a brown dog is a black collar running across the\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 12 ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "data_folder = './processed_data'  \n",
    "embedding_len = 512  \n",
    "attention_len = 512  \n",
    "lstm_len = 512  \n",
    "caps_per_image=2\n",
    "batch_size = 32\n",
    "learning_rate = 4e-4  \n",
    "\n",
    "# Read word map\n",
    "word_map_file = os.path.join(data_folder, 'WORDMAP.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "# Decoder model\n",
    "decoder = Decoder(attention_len=attention_len,\n",
    "                    embedding_len=embedding_len,\n",
    "                    features_len=lstm_len,\n",
    "                    wordmap_len=len(word_map))\n",
    "# We need an optimiser to update the model weights\n",
    "grad_params=filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "decoder_optimizer = torch.optim.Adam(params=grad_params, lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dataloaders are wrappers around datasets that help woth the learning.\n",
    "# Its not mandatory but its usefull so we might as well use it\n",
    "dataset=FlickrDataset(data_folder, 'DEV', caps_per_image)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "            FlickrDataset(data_folder, 'DEV', caps_per_image),\n",
    "            batch_size=5, )\n",
    "\n",
    "losses=[]\n",
    "weights=[]\n",
    "predictions=[]\n",
    "print('--------------------------------------------- LEARNING STARTED -------------------------------------------')\n",
    "for epoch in range(1,15):\n",
    "    print( '----------------------------------- Epoch',epoch,'----------------------------------')\n",
    "    for i,(imgs,caps,caplens) in enumerate(data_loader):\n",
    "        #print( '----------------------------------- Batch',i,'----------------------------------')\n",
    "\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind=decoder.forward(imgs, caps.to(dtype=torch.int64), caplens)\n",
    "        # Remove the <start> word\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        prediction=''\n",
    "        for line in scores[0]:\n",
    "            idx=line.argmax()\n",
    "            for key in word_map.keys():\n",
    "                if word_map[key]==idx:\n",
    "                    prediction+=' '+str(key)\n",
    "        caption=''\n",
    "        for line in caps[sort_ind[0], 1:caplens[sort_ind[0]]+1]:\n",
    "            for key in word_map.keys():\n",
    "                if word_map[key]==line:\n",
    "                    caption+=' '+str(key)\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "        #print('loss : ',loss.item())\n",
    "        decoder_optimizer.zero_grad()\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        decoder_optimizer.step()\n",
    "    predictions.append(prediction)\n",
    "    weights.append(alphas)    \n",
    "    #print('--------------------------------------------------------------')\n",
    "    print('Prediction : ',prediction)\n",
    "    print('Truth : ',caption)\n",
    "    #print('--------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
