{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- LEARNING STARTED -------------------------------------------\n",
      "----------------------------------- Epoch 1 ----------------------------------\n",
      "processed_data\\flickr\\TRAIN_images\\1001773457_577c3a7d70.pt  <start> a black dog and a spotted dog are fighting <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1003163366_44323f5815.pt  <start> a man lays on the bench to which a white dog is also tied <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1009434119_febe49276a.pt  <start> a black and white dog is running in a grassy garden surrounded by a white fence <end> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1015118661_980735411b.pt  <start> a little boy is standing on the street while a man in overalls is working on a stone wall <end> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\101669240_b2d3e7f17b.pt  <start> a man in a hat is displaying pictures next to a skier in a blue hat <end> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1019077836_6fc9b15408.pt  <start> a brown dog plays with the hose <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1022454428_b6b660a67b.pt  <start> a couple and an infant being held by the male sitting next to a pond with a near by stroller <end>\n",
      "processed_data\\flickr\\TRAIN_images\\102351840_323e3de834.pt  <start> a man is drilling through the frozen ice of a pond <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1026685415_0431cbf574.pt  <start> a black dog carries a green toy in his mouth as he walks through the grass <end> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1030985833_b0902ea560.pt  <start> a black dog and a brown dog play with a red toy on a courtyard <end> <pad> <pad> <pad> <pad> <pad>\n",
      "processed_data\\flickr\\TRAIN_images\\1032460886_4a598ed535.pt  <start> a man is standing in front of a skyscraper <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 22 and 25 in dimension 1 at c:\\a\\w\\1\\s\\windows\\pytorch\\aten\\src\\th\\generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-277f6b942f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'----------------------------------- Epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'----------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcaps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcaplens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mcaption\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcaps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 22 and 25 in dimension 1 at c:\\a\\w\\1\\s\\windows\\pytorch\\aten\\src\\th\\generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "data_folder = os.path.join('processed_data','flickr')\n",
    "embedding_len = 512  \n",
    "attention_len = 512  \n",
    "lstm_len = 512  \n",
    "caps_per_image=2\n",
    "batch_size = 32\n",
    "learning_rate = 4e-3  \n",
    "\n",
    "# Read word map\n",
    "word_map_file = os.path.join(data_folder, 'WORDMAP.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "# Decoder model\n",
    "decoder = Decoder(attention_len=attention_len,\n",
    "                    embedding_len=embedding_len,\n",
    "                    features_len=lstm_len,\n",
    "                    wordmap_len=len(word_map))\n",
    "# We need an optimiser to update the model weights\n",
    "grad_params=filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "decoder_optimizer = torch.optim.Adam(params=grad_params, lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dataloaders are wrappers around datasets that help woth the learning.\n",
    "# Its not mandatory but its usefull so we might as well use it\n",
    "dataset=FlickrDataset(data_folder, 'TRAIN', caps_per_image)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "            FlickrDataset(data_folder, 'TRAIN', caps_per_image),\n",
    "            batch_size=5, )\n",
    "\n",
    "losses=[]\n",
    "weights=[]\n",
    "predictions=[]\n",
    "print('--------------------------------------------- LEARNING STARTED -------------------------------------------')\n",
    "for epoch in range(1,5):\n",
    "    print( '----------------------------------- Epoch',epoch,'----------------------------------')\n",
    "    for i,(imgs,caps,caplens) in enumerate(data_loader):\n",
    "        caption=''\n",
    "        for line in caps[2]:\n",
    "            for key in word_map.keys():\n",
    "                if word_map[key]==line:\n",
    "                    caption+=' '+str(key)\n",
    "        print(imgs[2],caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- LEARNING STARTED -------------------------------------------\n",
      "----------------------------------- Epoch 1 ----------------------------------\n",
      "Loss :  tensor(8.0659, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(7.4355, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(6.5148, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(5.2993, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(5.9644, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(6.4915, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(6.0818, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(4.8781, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a in in in the in <end> <end> <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 2 ----------------------------------\n",
      "Loss :  tensor(4.6901, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(4.3073, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.9062, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.7140, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(4.0513, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(4.4262, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(4.4263, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.7345, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a a dog dog dog dog dog dog snow dog snow dog\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 3 ----------------------------------\n",
      "Loss :  tensor(3.8989, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.6900, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.6272, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.2018, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.5016, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.8400, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.7850, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.0096, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a dog dog dog a dog a <end> <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 4 ----------------------------------\n",
      "Loss :  tensor(3.2202, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.2840, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.2931, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.9039, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.1877, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.2971, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.3808, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.6080, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown brown brown dog a beach <end> <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 5 ----------------------------------\n",
      "Loss :  tensor(2.6638, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.8656, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.8305, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.5165, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.7223, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(3.1078, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.8961, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.1813, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog dog dog dog <end> <end> <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 6 ----------------------------------\n",
      "Loss :  tensor(2.3762, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.4871, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.5132, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.2962, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.1862, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.5550, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.5537, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9498, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown brown dog dog a beach beach beach beach beach <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 7 ----------------------------------\n",
      "Loss :  tensor(2.1715, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0323, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.2442, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.1262, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9570, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.2086, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.5219, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7718, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog a a a <end> <end> <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 8 ----------------------------------\n",
      "Loss :  tensor(2.0846, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0703, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.1870, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9883, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5512, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.2218, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.3324, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.6763, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown brown dog dog a a running <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 9 ----------------------------------\n",
      "Loss :  tensor(2.1184, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7663, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0362, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8219, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0345, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.3904, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5548, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog a a a collar beach beach beach beach <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 10 ----------------------------------\n",
      "Loss :  tensor(2.2300, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8967, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9304, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7967, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.6435, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.1118, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.2145, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4821, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog is running running running running <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 11 ----------------------------------\n",
      "Loss :  tensor(2.0837, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0264, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0549, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8717, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4532, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0816, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.2518, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog dog wearing a a a beach beach beach beach\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 12 ----------------------------------\n",
      "Loss :  tensor(2.0754, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9254, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9224, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8155, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5713, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9918, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.1223, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5004, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog running running running running <end> <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 13 ----------------------------------\n",
      "Loss :  tensor(1.9251, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.6356, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8827, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7992, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5637, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9215, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.2602, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5278, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog dog a a a a a a <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 14 ----------------------------------\n",
      "Loss :  tensor(1.9903, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7327, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7276, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8227, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8624, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0860, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5327, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown is running across across across across across across across across\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 15 ----------------------------------\n",
      "Loss :  tensor(1.8466, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.5925, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.8311, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7877, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.3029, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7109, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0283, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog is a a running running <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 16 ----------------------------------\n",
      "Loss :  tensor(1.8449, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4489, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7674, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7724, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.0326, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7161, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(2.0576, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4376, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog wearing wearing black beach beach beach beach beach beach\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 17 ----------------------------------\n",
      "Loss :  tensor(1.7614, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7178, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7798, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.6636, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4166, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7735, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.9519, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.3105, grad_fn=<NllLossBackward>)\n",
      "Prediction :   a brown dog is a a collar running <end> <end> <end> <end>\n",
      "Truth :   a brown dog wearing a black collar running across the beach\n",
      "----------------------------------- Epoch 18 ----------------------------------\n",
      "Loss :  tensor(1.7742, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.4219, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.6901, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.6944, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.0142, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.6872, grad_fn=<NllLossBackward>)\n",
      "Loss :  tensor(1.7686, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "data_folder = os.path.join('processed_data','flickr')\n",
    "embedding_len = 512  \n",
    "attention_len = 512  \n",
    "lstm_len = 512  \n",
    "caps_per_image=2\n",
    "batch_size = 32\n",
    "learning_rate = 4e-3  \n",
    "\n",
    "# Read word map\n",
    "word_map_file = os.path.join(data_folder, 'WORDMAP.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "# Decoder model\n",
    "decoder = Decoder(attention_len=attention_len,\n",
    "                    embedding_len=embedding_len,\n",
    "                    features_len=lstm_len,\n",
    "                    wordmap_len=len(word_map))\n",
    "# We need an optimiser to update the model weights\n",
    "grad_params=filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "decoder_optimizer = torch.optim.Adam(params=grad_params, lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dataloaders are wrappers around datasets that help woth the learning.\n",
    "# Its not mandatory but its usefull so we might as well use it\n",
    "dataset=FlickrDataset(data_folder, 'DEV', caps_per_image)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "            FlickrDataset(data_folder, 'DEV', caps_per_image),\n",
    "            batch_size=5, )\n",
    "\n",
    "losses=[]\n",
    "weights=[]\n",
    "predictions=[]\n",
    "print('--------------------------------------------- LEARNING STARTED -------------------------------------------')\n",
    "for epoch in range(1,50):\n",
    "    print( '----------------------------------- Epoch',epoch,'----------------------------------')\n",
    "    for i,(imgs,caps,caplens) in enumerate(data_loader):\n",
    "        #print( '----------------------------------- Batch',i,'----------------------------------')\n",
    "\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind=decoder.forward(imgs, caps.to(dtype=torch.int64), caplens)\n",
    "        # Remove the <start> word\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        prediction=''\n",
    "        for line in scores[0]:\n",
    "            idx=line.argmax()\n",
    "            for key in word_map.keys():\n",
    "                if word_map[key]==idx:\n",
    "                    prediction+=' '+str(key)\n",
    "        caption=''\n",
    "        for line in caps[sort_ind[0], 1:caplens[sort_ind[0]]+1]:\n",
    "            for key in word_map.keys():\n",
    "                if word_map[key]==line:\n",
    "                    caption+=' '+str(key)\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "        #print('loss : ',loss.item())\n",
    "        decoder_optimizer.zero_grad()\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        decoder_optimizer.step()\n",
    "        print('Loss : ',loss)\n",
    "    predictions.append(prediction)\n",
    "    weights.append(alphas)    \n",
    "    #print('--------------------------------------------------------------')\n",
    "    print('Prediction : ',prediction)\n",
    "    print('Truth : ',caption)\n",
    "    #print('--------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses=np.array(losses)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.transform import resize\n",
    "from skimage import io, transform\n",
    "\n",
    "path='./raw_data/flickr_data/488416045_1c6d903fe0.jpg'\n",
    "image = io.imread(path)\n",
    "\n",
    "for i,weight in enumerate(weights):\n",
    "    plt.figure(i,figsize=(8, 6))\n",
    "    for j,word in  enumerate(weight[-1]):\n",
    "        attention=np.array(word.detach()).reshape(14,14)\n",
    "        attention = resize(attention, (224, 224))\n",
    "        plt.subplot(1,len(weight[-1]),j+1)\n",
    "        plt.imshow(image)\n",
    "        plt.subplots_adjust(hspace = .1)\n",
    "\n",
    "        plt.imshow(attention,cmap='gray', alpha=1)\n",
    "        plt.title(predictions[i].split()[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
