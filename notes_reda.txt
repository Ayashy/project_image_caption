### Regularisation

- I removed dropout Regularisation intill further enquiry

### Initial lstm hidden state

- We have 3 options: Initialise to 0 - Learn the first weights - Use random noise
- This article goes into detail about it : https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html
- Will use zero tensor for now.

### Gating the attention_encodings

- It is recommanded by the authors for some reason?!
- Its a sigmoid(linear(Ht-1))*attention_encodings




############

Teacher forcing :
- Percentage 30%
- Use max score from previous

Dropout :
- Only if overfitting
- we'll see

Attention : 
- test without
- 15% gains

Validation :
- Learning rate

Embedding:
- min freq 3?
- use end of sentence

Visdom?